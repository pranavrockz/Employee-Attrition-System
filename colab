# -*- coding: utf-8 -*-
"""Minor_HR_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cdm504Q27N5YUjWoA1jS3eOcdHPAIMuh
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder
import glob
import os

from sklearn.datasets import make_classification

# Generate correlated features with binary target
X, y = make_classification(
    n_samples=100000,          # number of rows
    n_features=20,            # total features
    n_informative=8,          # truly correlated features
    n_redundant=4,            # linear combos of informative ones
    n_repeated=0,
    n_classes=2,              # binary target (Attrition)
    class_sep=1.0,            # separation between classes (lower = more overlap)
    flip_y=0.03,              # noise in labels (real-world randomness)
    random_state=42
)

columns = [
    'Age', 'MonthlyIncome', 'DistanceFromHome', 'YearsAtCompany', 'TrainingTimesLastYear',
    'JobSatisfaction', 'WorkLifeBalance', 'PerformanceRating', 'NumCompaniesWorked',
    'EducationLevel', 'OvertimeHours', 'Bonus', 'Tenure', 'Absences',
    'CommuteTime', 'ProjectsHandled', 'LeadershipScore', 'EngagementScore',
    'ManagerRating', 'PromotionRate'
]

df = pd.DataFrame(X, columns=columns)
df['Attrition'] = y

# Make values realistic
df['MonthlyIncome'] = (df['MonthlyIncome'] - df['MonthlyIncome'].min()) * 8000 + 30000
df['YearsAtCompany'] = (df['YearsAtCompany'] - df['YearsAtCompany'].min()) * 2 + 5
df['DistanceFromHome'] = abs(df['DistanceFromHome']) * 10
df['JobSatisfaction'] = np.clip(df['JobSatisfaction'], 1, 5).round()
df['WorkLifeBalance'] = np.clip(df['WorkLifeBalance'], 1, 4).round()
df['EducationLevel'] = np.clip(df['EducationLevel'], 1, 4).round()

# Add categorical columns
departments = ['IT', 'HR', 'Sales', 'Finance', 'Operations', 'R&D']
df['Department'] = np.random.choice(departments, len(df))

genders = ['Male', 'Female']
df['Gender'] = np.random.choice(genders, len(df))

marital_status = ['Single', 'Married', 'Divorced']
df['MaritalStatus'] = np.random.choice(marital_status, len(df))

import numpy as np
import pandas as pd

np.random.seed(42)
n = 100000

# Correlated numeric data
age = np.random.normal(38, 10, n).astype(int)
years_at_company = np.clip((age - 22) * np.random.uniform(0.2, 0.8, n), 0, 40).astype(int)
monthly_income = (2000 + years_at_company * np.random.uniform(400, 800, n)).astype(int)
job_level = np.clip((years_at_company // 8) + np.random.randint(1, 4, n), 1, 5)
total_working_years = years_at_company + np.random.randint(0, 10, n)
distance_from_home = np.random.randint(1, 50, n)
training_times_last_year = np.random.randint(1, 6, n)
performance_rating = np.random.choice([1, 2, 3, 4], n, p=[0.05, 0.15, 0.6, 0.2])
work_life_balance = np.random.choice([1, 2, 3, 4], n, p=[0.1, 0.2, 0.5, 0.2])
num_companies_worked = np.random.randint(1, 10, n)
bonus = np.random.randint(1000, 10000, n)
job_satisfaction = np.random.choice([1, 2, 3, 4], n, p=[0.1, 0.2, 0.5, 0.2])
relationship_satisfaction = np.random.choice([1, 2, 3, 4], n, p=[0.1, 0.2, 0.5, 0.2])

# Categorical columns
department = np.random.choice(['HR', 'Sales', 'R&D'], n, p=[0.2, 0.4, 0.4])
education_field = np.random.choice(['Life Sciences', 'Technical', 'Medical', 'Marketing'], n)
marital_status = np.random.choice(['Single', 'Married', 'Divorced'], n, p=[0.3, 0.5, 0.2])
business_travel = np.random.choice(['Travel_Rarely', 'Travel_Frequently', 'Non-Travel'], n, p=[0.6, 0.3, 0.1])
gender = np.random.choice(['Male', 'Female'], n, p=[0.55, 0.45])
overtime = np.random.choice(['Yes', 'No'], n, p=[0.3, 0.7])
job_role = np.random.choice(['Manager', 'Sales Executive', 'Research Scientist', 'Lab Technician', 'HR'], n)

# Attrition logic
attrition_prob = (
    0.25 * (overtime == 'Yes').astype(int)
    + 0.2 * (job_satisfaction <= 2).astype(int)
    + 0.15 * (performance_rating <= 2).astype(int)
    + 0.15 * (distance_from_home > 30).astype(int)
    + 0.1 * (work_life_balance <= 2).astype(int)
)
attrition = np.where(attrition_prob + np.random.uniform(0, 0.3, n) > 0.5, 'Yes', 'No')

# Build DataFrame
df = pd.DataFrame({
    'Age': age,
    'Attrition': attrition,
    'BusinessTravel': business_travel,
    'Department': department,
    'DistanceFromHome': distance_from_home,
    'Education': np.random.randint(1, 6, n),
    'EducationField': education_field,
    'EmployeeCount': 1,
    'EmployeeNumber': np.arange(1, n + 1),
    'EnvironmentSatisfaction': np.random.choice([1, 2, 3, 4], n),
    'Gender': gender,
    'HourlyRate': np.random.randint(30, 200, n),
    'JobInvolvement': np.random.choice([1, 2, 3, 4], n),
    'JobLevel': job_level,
    'JobRole': job_role,
    'JobSatisfaction': job_satisfaction,
    'MaritalStatus': marital_status,
    'MonthlyIncome': monthly_income,
    'Bonus': bonus,
    'NumCompaniesWorked': num_companies_worked,
    'OverTime': overtime,
    'PerformanceRating': performance_rating,
    'RelationshipSatisfaction': relationship_satisfaction,
    'StockOptionLevel': np.random.randint(0, 5, n),
    'TotalWorkingYears': total_working_years,
    'TrainingTimesLastYear': training_times_last_year,
    'WorkLifeBalance': work_life_balance,
    'YearsAtCompany': years_at_company
})

# Introduce missing values (~5%)
for col in df.columns:
    if df[col].dtype != 'object':
        df.loc[df.sample(frac=0.05).index, col] = np.nan

# Introduce outliers in selected numeric columns
outlier_cols = ['MonthlyIncome', 'Bonus', 'DistanceFromHome', 'Age']
for col in outlier_cols:
    n_outliers = int(0.01 * n)
    outlier_indices = np.random.choice(df.index, n_outliers, replace=False)
    df.loc[outlier_indices, col] *= np.random.uniform(3, 10, n_outliers)

print(df.info())
print(df.describe(include='all').T)

df.to_csv("HR_Analytics_Combined.csv", index=False)

print(df.head())
df.shape

df.info()

df.describe()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Check for missing values
print("Missing values before handling:\n")

missing_summary = (
    df.isnull().sum()
    .reset_index()
    .rename(columns={'index': 'Column', 0: 'MissingValues'})
)
missing_summary['% Missing'] = (missing_summary['MissingValues'] / len(df)) * 100
missing_summary = missing_summary.sort_values(by='MissingValues', ascending=False)

# Display summary
print(missing_summary.to_string(index=False))
print(f"\n Total missing values: {df.isnull().sum().sum()}")

# --- Plot missing values heatmap ---
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# --- Separate numerical and categorical columns ---
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"\n Numerical columns: {len(numerical_cols)}")
print(f" Categorical columns: {len(categorical_cols)}")

from sklearn.impute import KNNImputer
import pandas as pd

# Group-based fill columns (values depend on department/jobrole)
group_cols = ['Department', 'JobRole']
group_fill_cols = [
    'JobSatisfaction', 'JobInvolvement', 'EnvironmentSatisfaction',
    'RelationshipSatisfaction', 'YearsAtCompany'
]

# Columns for KNN imputation (continuous)
knn_fill_cols = [
    'Age', 'DistanceFromHome', 'MonthlyIncome', 'HourlyRate',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'Bonus'
]

for col in group_fill_cols:
    group_medians = df.groupby(group_cols)[col].transform('median')
    df[col] = df[col].fillna(group_medians)
    df[col] = df[col].fillna(df[col].median())  # Fallback to global median

# Create a copy for KNN imputation
knn_df = df[knn_fill_cols]

# Initialize KNN Imputer
imputer = KNNImputer(n_neighbors=5, weights='uniform')

# Fit and transform the data
knn_imputed = imputer.fit_transform(knn_df)

# Replace back into original DataFrame
df[knn_fill_cols] = pd.DataFrame(knn_imputed, columns=knn_fill_cols)

df.isnull().sum()

group_cols = ['Department', 'JobRole']

group_fill_cols = [
    'Education',
    'JobLevel',
    'PerformanceRating',
    'WorkLifeBalance'
]

for col in group_fill_cols:
    df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(x.median()))

knn_cols = [
    'EmployeeCount',
    'EmployeeNumber',
    'NumCompaniesWorked',
    'StockOptionLevel'
]
# Apply on selected numeric columns
df[knn_cols] = imputer.fit_transform(df[knn_cols])

df.isnull().sum()

# Detect Outliers
# Method 1: Statistical (IQR)
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Method 2: Z-score
from scipy import stats
def detect_outliers_zscore(df, column, threshold=3):
    z_scores = np.abs(stats.zscore(df[column]))
    outliers = df[z_scores > threshold]
    return outliers

# Method 3: Visual detection
def plot_outliers(df, numerical_cols, n_cols=4):
    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))
    axes = axes.flatten()

    for i, col in enumerate(numerical_cols):
        if i < len(axes):
            df.boxplot(column=col, ax=axes[i])
            axes[i].set_title(f'Boxplot of {col}')

    # Hide empty subplots
    for j in range(i+1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

# Plot outliers for numerical columns
plot_outliers(df, numerical_cols)

def detect_outliers_iqr(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
    return df[(df[col] < lower) | (df[col] > upper)]

for col in ['MonthlyIncome', 'Bonus', 'DistanceFromHome', 'Age']:
    outliers = detect_outliers_iqr(df, col)
    print(f"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)")

import numpy as np

def cap_outliers(df, col, lower_percentile=0.01, upper_percentile=0.99):
    lower = df[col].quantile(lower_percentile)
    upper = df[col].quantile(upper_percentile)
    df[col] = np.where(df[col] < lower, lower, df[col])
    df[col] = np.where(df[col] > upper, upper, df[col])
    return df

# Cap where needed
df = cap_outliers(df, 'MonthlyIncome')
df = cap_outliers(df, 'Bonus')
df = cap_outliers(df, 'DistanceFromHome')

# Clip age between 18 and 65
df['Age'] = df['Age'].clip(lower=18, upper=65)

# Re-check for outliers
def count_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
    return len(df[(df[col] < lower) | (df[col] > upper)])

for col in ['MonthlyIncome', 'Bonus', 'DistanceFromHome', 'Age']:
    print(f"{col}: {count_outliers(df, col)} outliers remaining")

# Optional: visualize cleaned distribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.boxplot(data=df[['MonthlyIncome', 'Bonus', 'DistanceFromHome', 'Age']])
plt.title("Boxplot After Outlier Treatment")
plt.show()

# Columns that are constant (no variance)
constant_cols = [col for col in df.columns if df[col].nunique() == 1]
print("Constant columns:", constant_cols)

# Drop if any
df.drop(columns=constant_cols, inplace=True)

def get_duplicate_columns(df):

    duplicate_columns = {}
    seen_columns = {}

    for column in df.columns:
        current_column = df[column]

        # Convert column data to bytes
        try:
            current_column_hash = current_column.values.tobytes()
        except AttributeError:
            current_column_hash = current_column.to_string().encode()

        if current_column_hash in seen_columns:
            if seen_columns[current_column_hash] in duplicate_columns:
                duplicate_columns[seen_columns[current_column_hash]].append(column)
            else:
                duplicate_columns[seen_columns[current_column_hash]] = [column]
        else:
            seen_columns[current_column_hash] = column

    return duplicate_columns

duplicate_columns = get_duplicate_columns(df)

duplicate_columns

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)

from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=0.05)

# 1. Find Numeric Columns
numeric_cols = df.select_dtypes(include=np.number).columns
num_numeric = len(numeric_cols)
print(f"Number of numeric columns: {num_numeric}")
print(f"Numeric columns: {list(numeric_cols)}\n")

plt.figure(figsize=(10, 6))
sns.heatmap(df[['Age' ,'DistanceFromHome', 'Education', 'EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'Bonus', 'NumCompaniesWorked', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany']]
            .corr(),
            annot=True, cmap='coolwarm', fmt=".2f")

plt.title("Correlation Heatmap of Numeric HR Features")
plt.show()

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Select only numeric columns for VIF computation
numeric_df = df.select_dtypes(include=['float64', 'int64']).copy()

# Drop identifier columns that donâ€™t represent real features
drop_cols = ['EmployeeNumber', 'EmployeeCount']  # You can add more if needed
numeric_df = numeric_df.drop(columns=drop_cols, errors='ignore')

# Compute VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = numeric_df.columns
vif_data['VIF'] = [
    variance_inflation_factor(numeric_df.values, i)
    for i in range(numeric_df.shape[1])
]

# Sort and display
vif_data = vif_data.sort_values(by='VIF', ascending=False).reset_index(drop=True)
print(vif_data)

df.drop(['Age', 'YearsAtCompany'], axis=1 , inplace = True)

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64']).copy()

# Drop rows with NaN values (VIF cannot handle missing)
numeric_df = numeric_df.dropna()

# Recompute VIF
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = numeric_df.columns
vif_data_reduced['VIF'] = [
    variance_inflation_factor(numeric_df.values, i)
    for i in range(numeric_df.shape[1])
]

# Sort and print
vif_data_reduced = vif_data_reduced.sort_values(by='VIF', ascending=False)
print(vif_data_reduced)

df['Attrition'] = df['Attrition'].astype(str).str.strip().str.title().map({'Yes': 1, 'No': 0})
df['Attrition'].value_counts()
df.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 2ï¸âƒ£ Define numerical columns
numerical_cols = [
    'DistanceFromHome', 'Education', 'EmployeeNumber', 'EnvironmentSatisfaction',
    'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome',
    'Bonus', 'NumCompaniesWorked', 'PerformanceRating', 'RelationshipSatisfaction',
    'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',
]

# 3ï¸âƒ£ Convert all numerical columns to numeric dtype (float)
for col in numerical_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)

# 4ï¸âƒ£ Compute correlation with Attrition (drop NaNs)
corr_with_attrition = (
    df[numerical_cols + ['Attrition']]
    .corr()['Attrition']
    .drop('Attrition')
    .sort_values(ascending=False)
)

# 5ï¸âƒ£ Print correlation values
print("\nðŸ”¹ Correlation of Numeric Features with Attrition:\n")
print(corr_with_attrition)

# 6ï¸âƒ£ Visualize correlations
plt.figure(figsize=(10, 8))
sns.barplot(x=corr_with_attrition.values, y=corr_with_attrition.index, palette='coolwarm')
plt.title('Correlation of Numeric Features with Attrition', fontsize=14)
plt.xlabel('Correlation Coefficient')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Compute correlation matrix
corr_matrix = df.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(
    corr_matrix,
    annot=False,
    cmap='coolwarm',
    linewidths=0.5,
    cbar=True,
    center=0
)

plt.title('Correlation Heatmap of HR Dataset', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True)['Attrition'].sort_values(ascending=False).to_frame(),
            annot=True, cmap='coolwarm')
plt.title('Feature Correlation with Attrition', fontsize=14)
plt.show()

df.head()

#Attrition rate by Department
pivot_dept = pd.pivot_table(df,
                            values='Attrition',
                            index='Department',
                            aggfunc=['mean', 'count'])
pivot_dept.columns = ['Attrition Rate', 'Employee Count']
pivot_dept = pivot_dept.sort_values('Attrition Rate', ascending=False)
print(pivot_dept)

# Attrition by JobRole and JobLevel
pivot_role_level = pd.pivot_table(df,
                                  values='Attrition',
                                  index='JobRole',
                                  columns='JobLevel',
                                  aggfunc='mean')
print(pivot_role_level)

# Attrition by WorkLifeBalance and JobSatisfaction
pivot_wlb_js = pd.pivot_table(df,
                              values='Attrition',
                              index='WorkLifeBalance',
                              columns='JobSatisfaction',
                              aggfunc='mean')
print(pivot_wlb_js)

# Average MonthlyIncome and Bonus by Attrition Status
pivot_income = pd.pivot_table(df,
                              values=['MonthlyIncome', 'Bonus'],
                              index='Attrition',
                              aggfunc='mean')
print(pivot_income)

pivot_ot = pd.pivot_table(df,
                          values='Attrition',
                          index='OverTime',
                          aggfunc='mean')
print(pivot_ot)

# ==============================================
# âœ… Unified Feature Selection Pipeline for Attrition Dataset
# ==============================================

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# 2ï¸âƒ£ Split features and target
X = df.drop(columns=['Attrition'])
y = df['Attrition']

# 3ï¸âƒ£ Encode categorical columns
cat_cols = X.select_dtypes(include=['object']).columns
X[cat_cols] = X[cat_cols].apply(LabelEncoder().fit_transform)

# 4ï¸âƒ£ Scale numeric columns
scaler = StandardScaler()
num_cols = X.select_dtypes(include=np.number).columns
X[num_cols] = scaler.fit_transform(X[num_cols])

print(f"âœ… Total Features: {X.shape[1]}")

# ===========================================================
# ðŸ“˜ (A) Filter Method â€” ANOVA F-test
# ===========================================================
select_kbest = SelectKBest(score_func=f_classif, k=10)
X_kbest = select_kbest.fit(X, y)
filter_features = X.columns[X_kbest.get_support()].tolist()
print("\nðŸ“Š Top 10 Features (Filter - ANOVA F-test):")
print(filter_features)

# ===========================================================
# ðŸ“— (B) Wrapper Method â€” Recursive Feature Elimination (RFE)
# ===========================================================
log_reg = LogisticRegression(max_iter=500)
rfe = RFE(log_reg, n_features_to_select=10)
rfe.fit(X, y)
wrapper_features = X.columns[rfe.support_].tolist()
print("\nðŸ” Top 10 Features (Wrapper - RFE):")
print(wrapper_features)

# ===========================================================
# ðŸ“™ (C) Embedded Method â€” Random Forest Importance
# ===========================================================
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X, y)

rf_importances = pd.Series(rf.feature_importances_, index=X.columns)
embedded_features = rf_importances.sort_values(ascending=False).head(10).index.tolist()
print("\nðŸŒ³ Top 10 Features (Embedded - Random Forest):")
print(embedded_features)

# ===========================================================
# ðŸ“ˆ Combine & Rank Features by Frequency of Selection
# ===========================================================
all_selected = filter_features + wrapper_features + embedded_features
feature_rank = pd.Series(all_selected).value_counts()

print("\nðŸ† Final Ranked Features (by frequency of selection):")
print(feature_rank)

# ===========================================================
# ðŸŽ¨ Visualization
# ===========================================================
plt.figure(figsize=(8,5))
sns.barplot(x=feature_rank.values, y=feature_rank.index, palette='coolwarm')
plt.title('Most Frequently Selected Features (Across Methods)', fontsize=14)
plt.xlabel('Selection Count')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# ===========================================================
# PCA for Dimensionality Reduction
# ===========================================================
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X)
explained = np.cumsum(pca.explained_variance_ratio_)
print("\nâš™ï¸ PCA Explained Variance (Cumulative):")
for i, val in enumerate(explained, 1):
    print(f"Component {i}: {val:.2%}")

categorical_cols

for i in categorical_cols:
  print(df[i].value_counts())
  print("\n")

import pandas as pd

cat_cols = ['BusinessTravel', 'Department', 'EducationField', 'Gender',
            'JobRole', 'MaritalStatus', 'OverTime']

# Attrition rate by category
for col in cat_cols:
    print(f"\n=== Attrition by {col} ===")
    print(df.groupby(col)['Attrition'].mean().sort_values(ascending=False))

# Frequency Encoding (good for trees)
for col in cat_cols:
    freq = df[col].value_counts(normalize=True)
    df[f'{col}_FreqEnc'] = df[col].map(freq)

# Target Encoding (average attrition rate per category)
for col in cat_cols:
    target_mean = df.groupby(col)['Attrition'].mean()
    df[f'{col}_TargetEnc'] = df[col].map(target_mean)

# Combine job and department insights
df['TechRole'] = df['JobRole'].apply(lambda x: 1 if 'Research' in x or 'Scientist' in x else 0)

# Combine travel and overtime
df['HighTravelOvertime'] = ((df['BusinessTravel'] == 'Travel_Frequently') &
                            (df['OverTime'] == 'Yes')).astype(int)

# Family + Work balance
df['SingleOvertime'] = ((df['MaritalStatus'] == 'Single') &
                        (df['OverTime'] == 'Yes')).astype(int)

# Department-based work intensity
df['SalesOrHR'] = df['Department'].apply(lambda x: 1 if x in ['Sales', 'HR'] else 0)

# Genderâ€“role bias indicator (e.g., high-stress roles)
df['MaleInSales'] = ((df['Gender'] == 'Male') & (df['Department'] == 'Sales')).astype(int)

new_cat_feats = ['TechRole', 'HighTravelOvertime', 'SingleOvertime',
                 'SalesOrHR', 'MaleInSales']

print("\nAttrition Rate by Newly Engineered Categorical Features:")
for col in new_cat_feats:
    rate = df.groupby(col)['Attrition'].mean()
    print(f"{col}:\n{rate}\n")

import seaborn as sns
import matplotlib.pyplot as plt

encoded_cols = [c for c in df.columns if c.endswith('_TargetEnc') or c in new_cat_feats]

plt.figure(figsize=(10,6))
sns.heatmap(df[encoded_cols + ['Attrition']].corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation of Encoded & Derived Categorical Features with Attrition')
plt.tight_layout()
plt.show()

# Drop uninformative engineered categorical features
drop_cols = ['TechRole', 'SalesOrHR', 'MaleInSales']

df = df.drop(columns=drop_cols, errors='ignore')

print("âœ… Dropped weak features:", drop_cols)
print("Remaining columns:", df.shape[1])

df.head()

df_label = df.copy()

df_hot = df.copy()

df_hot = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
print("âœ… One-hot encoding completed. New shape:", df_hot.shape)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in categorical_cols:
    df_label[col] = le.fit_transform(df[col].astype(str))

df_label.drop(['EmployeeNumber'] , axis = 1)
df_hot.drop(['EmployeeNumber'] , axis = 1)

from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

# 2ï¸âƒ£ Initialize the scaler
scaler = StandardScaler()

# 3ï¸âƒ£ Fit and transform the numeric features
df_label[numerical_cols] = scaler.fit_transform(df_label[numerical_cols])

# 4ï¸âƒ£ Verify scaling result
print("\nâœ… Scaling Complete! Summary:")
summary = pd.DataFrame({
    'Mean': df_label[numerical_cols].mean().round(2),
    'StdDev': df_label[numerical_cols].std().round(2)
})

df_label

X = df_label.drop('Attrition' , axis = 1)
y = df_label['Attrition']

print(X.shape)
print(y.shape)

from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.ensemble import RandomForestClassifier

Random_forest_model = RandomForestClassifier()
Random_forest_model.fit(X_train , y_train)

y_pred = Random_forest_model.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)
print(cm)

sns.heatmap(cm , annot = True , fmt='d')

from sklearn.metrics import accuracy_score
model_accuracy = accuracy_score(y_test,y_pred)
print("Accuracy of the model =" , model_accuracy)

from sklearn.metrics import precision_score
model_precision = precision_score(y_test,y_pred)
print("Precision of the model =" , model_precision)

from sklearn.metrics import recall_score
model_recall = recall_score(y_test,y_pred)
print("Recall of the model =" , model_recall)

from sklearn.metrics import f1_score
model_f1 = f1_score(y_test,y_pred)
print("F1 score of the model =" , model_f1)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

score_list = Random_forest_model.feature_importances_
list_of_features = list(X.columns)
score_df = pd.DataFrame({"Feature" : list_of_features , 'Score' : score_list})
score_df.sort_values(by = 'Score' , ascending = False)

list_of_features = list(X.columns)
plt.figure(figsize = (8,6))
plt.barh(range(len(list_of_features)) , Random_forest_model.feature_importances_)
plt.yticks(np.arange(len(list_of_features)),list_of_features)
plt.ylabel('Features')
plt.show()

from sklearn.model_selection import cross_val_score
scores = cross_val_score(Random_forest_model , X_train , y_train , cv = 5 , scoring='accuracy')
print('Cross-Validation_scores = ' , scores)

Avg_Model_score = scores.mean()
print("Average Model Score = " , Avg_Model_score)

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': [5, 10, 15, None],
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 4),
    'bootstrap': [True, False]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=20,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    random_state=42
)
random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best F1 Score:", random_search.best_score_)

from sklearn.ensemble import RandomForestClassifier

best_rf = RandomForestClassifier(
    bootstrap=False,
    max_depth=None,
    min_samples_leaf=1,
    min_samples_split=4,
    n_estimators=150,
    random_state=42
)

best_rf.fit(X_train, y_train)

y_pred = best_rf.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

xgb_model = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=42
)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1],
    'gamma': [0, 1],
    'reg_lambda': [1, 3, 5]
}

grid_search = GridSearchCV(
    estimator=XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        random_state=42,
        use_label_encoder=False
    ),
    param_grid=param_grid,
    cv=3,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best F1 Score:", grid_search.best_score_)

best_xgb = XGBClassifier(
    colsample_bytree = 1,
    gamma = 0,
    learning_rate = 0.05,
    max_depth =  5,
    n_estimators = 100,
    reg_lambda = 5,
    subsample = 1,
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=42,
)

best_xgb.fit(X_train, y_train)
y_pred = best_xgb.predict(X_test)

print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Build the model
model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dense(1, activation='sigmoid')
])

# Compile
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=256,
    callbacks=[early_stop],
    verbose=1
)

model.summary()

from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np

class KerasNN(BaseEstimator, ClassifierMixin):
    """Custom Keras wrapper compatible with VotingClassifier."""
    _estimator_type = "classifier"

    def __init__(self, build_fn, epochs=25, batch_size=256, verbose=0):
        self.build_fn = build_fn
        self.epochs = epochs
        self.batch_size = batch_size
        self.verbose = verbose
        self.model = None

    def fit(self, X, y):
        self.model = self.build_fn()
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)
        return self

    def predict(self, X):
        preds = (self.model.predict(X) > 0.5).astype("int32").ravel()
        return preds

    def predict_proba(self, X):
        proba = self.model.predict(X)
        return np.hstack([1 - proba, proba])

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.estimator_type = "classifier"
        tags.requires_fit = True
        tags.non_deterministic = True
        return tags

import numpy as np
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from scikeras.wrappers import KerasClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ------------------------------
# ðŸ”¹ Build the Neural Network
# ------------------------------
def build_nn():
    model = Sequential([
        Dense(256, activation='relu', input_dim=X_train_res.shape[1]),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Wrap Keras model for scikit-learn
nn = KerasNN(build_fn=build_nn, epochs=25, batch_size=256, verbose=0)

# ------------------------------
# ðŸŒ² Random Forest
# ------------------------------
rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=18,
    min_samples_split=4,
    min_samples_leaf=2,
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)

# ------------------------------
# ðŸš€ XGBoost
# ------------------------------
xgb = XGBClassifier(
    n_estimators=600,
    learning_rate=0.02,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.2,
    reg_lambda=2,
    reg_alpha=1,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)

ensemble = VotingClassifier(
    estimators=[('xgb', xgb), ('rf', rf), ('nn', nn)],
    voting='soft',
    weights=[3, 2, 3]  # Tune based on validation performance
)

ensemble.fit(X_train_res, y_train_res)


# ------------------------------
# ðŸ“Š Evaluate Ensemble
# ------------------------------
y_pred_ens = ensemble.predict(X_test)

acc = accuracy_score(y_test, y_pred_ens)
print(f"\nâœ… Ensemble Accuracy: {acc:.4f}\n")
print("ðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred_ens))
print("\nðŸ”¹ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ens))

# ==========================================
# ðŸ§  STACKING ENSEMBLE: XGBoost + RF + NN
# ==========================================
import numpy as np
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from scikeras.wrappers import KerasClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ------------------------------
# ðŸ”¹ Build Neural Network
# ------------------------------
from tensorflow.keras.callbacks import EarlyStopping

def build_nn():
    model = Sequential([
        Input(shape=(X_train_res.shape[1],)),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ------------------------------
# âš™ï¸ Define Base Models
# ------------------------------
nn = KerasClassifier(
    model=build_nn,
    epochs=50,
    batch_size=256,
    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
    verbose=0
)

rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=18,
    min_samples_split=4,
    min_samples_leaf=2,
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)

xgb = XGBClassifier(
    n_estimators=600,
    learning_rate=0.02,
    max_depth=7,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.2,
    reg_lambda=2,
    reg_alpha=1,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)

# ------------------------------
# ðŸ§© Build Stacking Ensemble
# ------------------------------
stacking_ensemble = StackingClassifier(
    estimators=[('xgb', xgb), ('rf', rf), ('nn', nn)],
    final_estimator=LogisticRegression(max_iter=1000, C=0.5, solver='lbfgs'),
    stack_method='predict_proba',
    n_jobs=-1,
    passthrough=True,  # Let meta-learner see original features + base model predictions
    cv=5
)

# ------------------------------
# ðŸ§± Add Scaling for NN
# ------------------------------
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("stack", stacking_ensemble)
])

# ------------------------------
# ðŸ‹ï¸ Train Ensemble
# ------------------------------
pipeline.fit(X_train_res, y_train_res)

# ------------------------------
# ðŸ“Š Evaluate Ensemble
# ------------------------------
y_pred_stack = pipeline.predict(X_test)

acc_stack = accuracy_score(y_test, y_pred_stack)
print(f"\nâœ… Stacking Ensemble Accuracy: {acc_stack:.4f}\n")
print("ðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred_stack))
print("\nðŸ”¹ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_stack))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# Predict on the test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['No Attrition', 'Attrition']))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

from joblib import dump, load
dump(Random_forest_model, 'random_forest_model.joblib')   # Saving to disk

# Load back later with:
rf_loaded = load('random_forest_model.joblib')

from joblib import dump, load

dump(best_xgb, '/content/xg_boost_model.joblib')

# Save
model.save('/content/neural_network_model.h5')

# Load
from tensorflow.keras.models import load_model
nn_loaded = load_model('neural_network_model.h5')
